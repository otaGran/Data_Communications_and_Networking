# Q1 :
An information source generates four messages m1, m2, m3 and m4 with probabilities of 1/2, 1/8, 1/8 and 1/4 respectively. Determine entropy of the system.
# Q2 : 
Determine the entropy of above example (question 1), if the probability of above messages (in question 1) are equally.
# Answer
 H = (1/4)log<sub>2</sub>(4) + (1/4)log<sub>2</sub>(4) + (1/4)log<sub>2</sub>(4) + (1/4)log<sub>2</sub>(4) <br>
  = 1/2 + 1/2 +1/2 + 1/2 <br>
  = 2 (bits/message).
